{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c909300b-d7fe-4a96-bd6a-9c256cc3dc3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip show transformers accelerate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cac5e0ef-fbfb-4d5e-83e4-a4bb708081c3",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%pip install --upgrade transformers accelerate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b54f889d-4d77-4f47-a713-470969315649",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import datasets\n",
    "from datasets import Dataset\n",
    "from copy import deepcopy\n",
    "import time\n",
    "from dataclasses import dataclass\n",
    "from typing import Dict, List, Optional, Tuple\n",
    "from datasets import load_dataset\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from accelerate import Accelerator\n",
    "import nltk\n",
    "from nltk.corpus import wordnet\n",
    "from sklearn.metrics import accuracy_score, f1_score\n",
    "from transformers import (\n",
    "    AutoTokenizer, \n",
    "    AutoImageProcessor,  \n",
    "    AutoModel,\n",
    "    TrainingArguments, \n",
    "    Trainer,\n",
    "    logging\n",
    ")\n",
    "\n",
    "# Disable MLflow logging (needed for sagemaker notebook)\n",
    "# os.environ['DISABLE_MLFLOW_INTEGRATION'] = 'TRUE'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29a493f2-f0c2-4002-8afd-0190c4fb1e6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read the answer space\n",
    "with open(os.path.join(\"dataset\", \"answer_space.txt\")) as f:\n",
    "    answer_space = f.read().splitlines()\n",
    "\n",
    "# Read the files and process the answer column\n",
    "train_df = pd.read_csv(os.path.join(\"dataset\", \"data_train.csv\"))\n",
    "test_df = pd.read_csv(os.path.join(\"dataset\", \"data_eval.csv\"))\n",
    "\n",
    "# Function to get first answer and its index\n",
    "def process_answer(answer):\n",
    "    first_answer = answer.replace(\" \", \"\").split(\",\")[0]\n",
    "    return answer_space.index(first_answer)\n",
    "\n",
    "# Apply the processing to both dataframes\n",
    "train_df['label'] = train_df['answer'].apply(process_answer)\n",
    "test_df['label'] = test_df['answer'].apply(process_answer)\n",
    "\n",
    "# Convert to datasets format\n",
    "original_dataset = datasets.DatasetDict({\n",
    "    'train': Dataset.from_pandas(train_df),\n",
    "    'test': Dataset.from_pandas(test_df)\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dad477ca-b814-40f9-8dec-f39552fb5dc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = original_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7eb31562-2715-4bef-834d-31f581eafa3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_rows_train = 8*100\n",
    "n_rows_test = 2*100\n",
    "\n",
    "# Select the first n rows from each set\n",
    "train_sample = original_dataset['train'].select(range(n_rows_train))\n",
    "test_sample = original_dataset['test'].select(range(n_rows_test))\n",
    "\n",
    "# Create a smaller DatasetDict with the samples\n",
    "small_dataset = {\n",
    "    \"train\": train_sample,\n",
    "    \"test\": test_sample\n",
    "}\n",
    "\n",
    "dataset = small_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "477665a8-3e7a-4f91-89f7-3aabdf7d76c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "nltk.data.path.append('./nltk_data')\n",
    "\n",
    "# Test: Access WordNet to confirm it works\n",
    "synsets = wordnet.synsets(\"dog\")\n",
    "print(synsets)\n",
    "# Disable MLflow logging\n",
    "os.environ['DISABLE_MLFLOW_INTEGRATION'] = 'TRUE'\n",
    "\n",
    "\n",
    "class MultimodalVQAModel(nn.Module):\n",
    "    def __init__(\n",
    "            self,\n",
    "            num_labels: int = len(answer_space),\n",
    "            intermediate_dim: int = 512,\n",
    "            pretrained_text_name: str = 'bert-base-uncased',\n",
    "            pretrained_image_name: str = 'google/vit-base-patch16-224-in21k'):\n",
    "\n",
    "        super(MultimodalVQAModel, self).__init__()\n",
    "        self.num_labels = num_labels\n",
    "        self.pretrained_text_name = pretrained_text_name\n",
    "        self.pretrained_image_name = pretrained_image_name\n",
    "\n",
    "        self.text_encoder = AutoModel.from_pretrained(\n",
    "            self.pretrained_text_name,\n",
    "            force_download=True\n",
    "        )\n",
    "        self.image_encoder = AutoModel.from_pretrained(\n",
    "            self.pretrained_image_name,\n",
    "            force_download=True\n",
    "        )\n",
    "        self.fusion = nn.Sequential(\n",
    "            nn.Linear(self.text_encoder.config.hidden_size + self.image_encoder.config.hidden_size, intermediate_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.5),\n",
    "        )\n",
    "\n",
    "        self.classifier = nn.Linear(intermediate_dim, self.num_labels)\n",
    "        self.criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "    def forward(\n",
    "            self,\n",
    "            input_ids: torch.LongTensor,\n",
    "            pixel_values: torch.FloatTensor,\n",
    "            attention_mask: Optional[torch.LongTensor] = None,\n",
    "            token_type_ids: Optional[torch.LongTensor] = None,\n",
    "            labels: Optional[torch.LongTensor] = None):\n",
    "\n",
    "        encoded_text = self.text_encoder(\n",
    "            input_ids=input_ids,\n",
    "            attention_mask=attention_mask,\n",
    "            token_type_ids=token_type_ids,\n",
    "            return_dict=True,\n",
    "        )\n",
    "        encoded_image = self.image_encoder(\n",
    "            pixel_values=pixel_values,\n",
    "            return_dict=True,\n",
    "        )\n",
    "        fused_output = self.fusion(\n",
    "            torch.cat(\n",
    "                [\n",
    "                    encoded_text['pooler_output'],\n",
    "                    encoded_image['pooler_output'],\n",
    "                ],\n",
    "                dim=1\n",
    "            )\n",
    "        )\n",
    "        logits = self.classifier(fused_output)\n",
    "\n",
    "        out = {\n",
    "            \"logits\": logits\n",
    "        }\n",
    "        if labels is not None:\n",
    "            loss = self.criterion(logits, labels)\n",
    "            out[\"loss\"] = loss\n",
    "\n",
    "        return out\n",
    "\n",
    "@dataclass\n",
    "class MultimodalCollator:\n",
    "    tokenizer: AutoTokenizer\n",
    "    preprocessor: AutoImageProcessor\n",
    "\n",
    "    def tokenize_text(self, texts: List[str]):\n",
    "        encoded_text = self.tokenizer(\n",
    "            text=texts,\n",
    "            padding='longest',\n",
    "            max_length=24,\n",
    "            truncation=True,\n",
    "            return_tensors='pt',\n",
    "            return_token_type_ids=True,\n",
    "            return_attention_mask=True,\n",
    "        )\n",
    "        return {\n",
    "            \"input_ids\": encoded_text['input_ids'].squeeze(),\n",
    "            \"token_type_ids\": encoded_text['token_type_ids'].squeeze(),\n",
    "            \"attention_mask\": encoded_text['attention_mask'].squeeze(),\n",
    "        }\n",
    "\n",
    "    def preprocess_images(self, images: List[str]):\n",
    "        processed_images = self.preprocessor(\n",
    "            images=[Image.open(os.path.join(\"dataset\", \"images\", image_id + \".png\")).convert('RGB') for image_id in images],\n",
    "            return_tensors=\"pt\",\n",
    "        )\n",
    "        return {\n",
    "            \"pixel_values\": processed_images['pixel_values'].squeeze(),\n",
    "        }\n",
    "\n",
    "    def __call__(self, raw_batch_dict):\n",
    "        return {\n",
    "            **self.tokenize_text(\n",
    "                raw_batch_dict['question']\n",
    "                if isinstance(raw_batch_dict, dict) else\n",
    "                [i['question'] for i in raw_batch_dict]\n",
    "            ),\n",
    "            **self.preprocess_images(\n",
    "                raw_batch_dict['image_id']\n",
    "                if isinstance(raw_batch_dict, dict) else\n",
    "                [i['image_id'] for i in raw_batch_dict]\n",
    "            ),\n",
    "            'labels': torch.tensor(\n",
    "                raw_batch_dict['label']\n",
    "                if isinstance(raw_batch_dict, dict) else\n",
    "                [i['label'] for i in raw_batch_dict],\n",
    "                dtype=torch.int64\n",
    "            ),\n",
    "        }\n",
    "\n",
    "def wup_measure(a,b,similarity_threshold=0.925):\n",
    "    \"\"\"\n",
    "    Returns Wu-Palmer similarity score.\n",
    "    More specifically, it computes:\n",
    "        max_{x in interp(a)} max_{y in interp(b)} wup(x,y)\n",
    "        where interp is a 'interpretation field'\n",
    "    \"\"\"\n",
    "    def get_semantic_field(a):\n",
    "        weight = 1.0\n",
    "        semantic_field = wordnet.synsets(a,pos=wordnet.NOUN)\n",
    "        return (semantic_field,weight)\n",
    "\n",
    "\n",
    "    def get_stem_word(a):\n",
    "        \"\"\"\n",
    "        Sometimes answer has form wordd+:wordid.\n",
    "        If so we return word and downweight\n",
    "        \"\"\"\n",
    "        weight = 1.0\n",
    "        return (a,weight)\n",
    "\n",
    "\n",
    "    global_weight=1.0\n",
    "\n",
    "    (a,global_weight_a)=get_stem_word(a)\n",
    "    (b,global_weight_b)=get_stem_word(b)\n",
    "    global_weight = min(global_weight_a,global_weight_b)\n",
    "\n",
    "    if a==b:\n",
    "        # they are the same\n",
    "        return 1.0*global_weight\n",
    "\n",
    "    if a==[] or b==[]:\n",
    "        return 0\n",
    "\n",
    "\n",
    "    interp_a,weight_a = get_semantic_field(a)\n",
    "    interp_b,weight_b = get_semantic_field(b)\n",
    "\n",
    "    if interp_a == [] or interp_b == []:\n",
    "        return 0\n",
    "\n",
    "    # we take the most optimistic interpretation\n",
    "    global_max=0.0\n",
    "    for x in interp_a:\n",
    "        for y in interp_b:\n",
    "            local_score=x.wup_similarity(y)\n",
    "            if local_score > global_max:\n",
    "                global_max=local_score\n",
    "\n",
    "    # we need to use the semantic fields and therefore we downweight\n",
    "    # unless the score is high which indicates both are synonyms\n",
    "    if global_max < similarity_threshold:\n",
    "        interp_weight = 0.1\n",
    "    else:\n",
    "        interp_weight = 1.0\n",
    "\n",
    "    final_score=global_max*weight_a*weight_b*interp_weight*global_weight\n",
    "    return final_score\n",
    "\n",
    "def batch_wup_measure(labels, preds):\n",
    "    wup_scores = [wup_measure(answer_space[label], answer_space[pred]) for label, pred in zip(labels, preds)]\n",
    "    return np.mean(wup_scores)\n",
    "\n",
    "def compute_metrics(eval_tuple: Tuple[np.ndarray, np.ndarray]) -> Dict[str, float]:\n",
    "    logits, labels = eval_tuple\n",
    "    preds = logits.argmax(axis=-1)\n",
    "    return {\n",
    "        \"wups\": batch_wup_measure(labels, preds),\n",
    "        \"acc\": accuracy_score(labels, preds),\n",
    "        \"f1\": f1_score(labels, preds, average='macro')\n",
    "    }\n",
    "\n",
    "def createMultimodalVQACollatorAndModel(text='bert-base-uncased', image='google/vit-base-patch16-224-in21k'):\n",
    "    tokenizer = AutoTokenizer.from_pretrained(text, force_download=True)\n",
    "    preprocessor = AutoImageProcessor.from_pretrained(image, force_download=True, use_fast=True )\n",
    "\n",
    "    multi_collator = MultimodalCollator(\n",
    "        tokenizer=tokenizer,\n",
    "        preprocessor=preprocessor,\n",
    "    )\n",
    "\n",
    "    multi_model = MultimodalVQAModel(pretrained_text_name=text, pretrained_image_name=image).to(device)\n",
    "    return multi_collator, multi_model\n",
    "\n",
    "def createAndTrainModel(dataset, args, text_model='bert-base-uncased',\n",
    "                       image_model='google/vit-base-patch16-224-in21k',\n",
    "                       multimodal_model='bert_vit'):\n",
    "    # Create output directory with timestamp to ensure uniqueness\n",
    "    timestamp = time.strftime(\"%Y%m%d-%H%M%S\")\n",
    "    output_dir = os.path.join(\"checkpoint\", multimodal_model, f\"run_{timestamp}\")\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "    collator, model = createMultimodalVQACollatorAndModel(text_model, image_model)\n",
    "\n",
    "    # Create a new TrainingArguments object with unique run name\n",
    "    training_args = TrainingArguments(\n",
    "        output_dir=output_dir,\n",
    "        run_name=f\"run_{timestamp}\",\n",
    "        evaluation_strategy=\"epoch\",\n",
    "        save_strategy=\"epoch\",\n",
    "        load_best_model_at_end=True,\n",
    "        metric_for_best_model=\"wups\",\n",
    "        greater_is_better=True,\n",
    "        per_device_train_batch_size=args.per_device_train_batch_size if hasattr(args, 'per_device_train_batch_size') else 8,\n",
    "        per_device_eval_batch_size=args.per_device_eval_batch_size if hasattr(args, 'per_device_eval_batch_size') else 8,\n",
    "        num_train_epochs=args.num_train_epochs if hasattr(args, 'num_train_epochs') else 3,\n",
    "        learning_rate=args.learning_rate if hasattr(args, 'learning_rate') else 5e-5,\n",
    "        report_to=[],\n",
    "        remove_unused_columns=False,  # Important: Keep all columns\n",
    "    )\n",
    "\n",
    "    # Verify dataset format\n",
    "    print(\"Sample training example:\", dataset['train'][0])\n",
    "    print(\"Sample test example:\", dataset['test'][0])\n",
    "\n",
    "    multi_trainer = Trainer(\n",
    "        model=model,\n",
    "        args=training_args,\n",
    "        train_dataset=dataset['train'],\n",
    "        eval_dataset=dataset['test'],\n",
    "        data_collator=collator,\n",
    "        compute_metrics=compute_metrics,\n",
    "    )\n",
    "\n",
    "    train_multi_metrics = multi_trainer.train()\n",
    "    eval_multi_metrics = multi_trainer.evaluate()\n",
    "\n",
    "    # Save the final model explicitly\n",
    "    model_save_path = os.path.join(output_dir, \"final_model.pt\")\n",
    "    torch.save({\n",
    "        'model_state_dict': model.state_dict(),\n",
    "        'train_metrics': train_multi_metrics,\n",
    "        'eval_metrics': eval_multi_metrics\n",
    "    }, model_save_path)\n",
    "\n",
    "    return collator, model, train_multi_metrics, eval_multi_metrics\n",
    "\n",
    "# Set device\n",
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "print(\"Device\")\n",
    "print(device)\n",
    "\n",
    "# Training arguments\n",
    "args = TrainingArguments(\n",
    "    output_dir=\"checkpoint\",\n",
    "    seed=42,\n",
    "    eval_strategy=\"steps\",\n",
    "    eval_steps=100,\n",
    "    logging_strategy=\"steps\",\n",
    "    logging_steps=100,\n",
    "    save_strategy=\"steps\",\n",
    "    save_steps=100,\n",
    "    save_total_limit=3,\n",
    "    metric_for_best_model='wups',\n",
    "    per_device_train_batch_size=16,\n",
    "    per_device_eval_batch_size=16,\n",
    "    remove_unused_columns=False,\n",
    "    num_train_epochs=5,\n",
    "    fp16=True,\n",
    "    dataloader_num_workers=8,\n",
    "    load_best_model_at_end=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81dfefc0-1829-4be7-9fcf-ce60cc1083cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "collator, model, train_multi_metrics, eval_multi_metrics = createAndTrainModel(dataset, args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "019cb495-e729-41bf-94fe-24cc2d2451a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = np.random.randint(len(answer_space), size=5)\n",
    "preds = np.random.randint(len(answer_space), size=5)\n",
    "\n",
    "def showAnswers(ids):\n",
    "    print([answer_space[id] for id in ids])\n",
    "\n",
    "showAnswers(labels)\n",
    "showAnswers(preds)\n",
    "\n",
    "print(\"Predictions vs Labels: \", batch_wup_measure(labels, preds))\n",
    "print(\"Labels vs Labels: \", batch_wup_measure(labels, labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf18b418-1fce-46ea-918b-de64033e0bdb",
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_multi_metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed14ec7f-cf15-4889-817a-5de2b427022e",
   "metadata": {},
   "source": [
    "# Load and test model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2bc019bc-30f8-494d-b6cf-ecee8673428e",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = MultimodalVQAModel()\n",
    "# RTC:checkpoint/bert_vit/run_20241116-153943/final_model.pt\n",
    "checkpoint_path = os.path.join(\"checkpoint\", \"bert_vit\", \"final\", \"full_64_64_10.pt\")\n",
    "if os.path.exists(checkpoint_path):\n",
    "    checkpoint = torch.load(checkpoint_path)\n",
    "    model.load_state_dict(checkpoint['model_state_dict'])\n",
    "    model.to(device)\n",
    "    print(\"Model loaded successfully\")\n",
    "    print(\"Final evaluation metrics:\", checkpoint['eval_metrics'])\n",
    "else:\n",
    "    print(\"No saved model found at:\", checkpoint_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5dbd01f4-d416-46c6-90bb-0a6292c25ad9",
   "metadata": {},
   "outputs": [],
   "source": [
    "sample = collator(dataset[\"test\"][100:1])\n",
    "\n",
    "input_ids = sample[\"input_ids\"].to(device)\n",
    "token_type_ids = sample[\"token_type_ids\"].to(device)\n",
    "attention_mask = sample[\"attention_mask\"].to(device)\n",
    "pixel_values = sample[\"pixel_values\"].to(device)\n",
    "labels = sample[\"labels\"].to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84c2f39e-e29d-437b-a8fc-d4df9cfd6b5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.eval()\n",
    "output = model(input_ids, pixel_values, attention_mask, token_type_ids, labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31d47dcf-3fbc-4c54-8c69-f2337c4ef4bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "preds = output[\"logits\"].argmax(axis=-1).cpu().numpy()\n",
    "preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "703ef3fe-fc27-4541-8dcd-1ff1cc77efdc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import display\n",
    "\n",
    "def showExample(train=True, id=None):\n",
    "    if train:\n",
    "        data = dataset[\"train\"]\n",
    "    else:\n",
    "        data = dataset[\"test\"]\n",
    "    if id == None:\n",
    "        id = np.random.randint(len(data))\n",
    "    image = Image.open(os.path.join(\"dataset\", \"images\", data[id][\"image_id\"] + \".png\"))\n",
    "    display(image)\n",
    "\n",
    "    print(\"Question:\\t\", data[id][\"question\"])\n",
    "    print(\"Answer:\\t\\t\", data[id][\"answer\"], \"(Label: {0})\".format(data[id][\"label\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5eefccf9-13f9-40eb-8f78-7258cb879780",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(000, 005):\n",
    "    print(\"*********************************************************\")\n",
    "    showExample(train=False, id=i)\n",
    "    print(\"Predicted Answer:\\t\", answer_space[preds[i-1000]])\n",
    "    print(\"*********************************************************\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8047953-2c9b-47bf-9cff-0397b8a916a3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
