{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c909300b-d7fe-4a96-bd6a-9c256cc3dc3d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Name: transformers\n",
      "Version: 4.38.2\n",
      "Summary: State-of-the-art Machine Learning for JAX, PyTorch and TensorFlow\n",
      "Home-page: https://github.com/huggingface/transformers\n",
      "Author: The Hugging Face team (past and future) with the help of all our contributors (https://github.com/huggingface/transformers/graphs/contributors)\n",
      "Author-email: transformers@huggingface.co\n",
      "License: Apache 2.0 License\n",
      "Location: /opt/conda/lib/python3.11/site-packages\n",
      "Requires: filelock, huggingface-hub, numpy, packaging, pyyaml, regex, requests, safetensors, tokenizers, tqdm\n",
      "Required-by: autogluon.multimodal, autogluon.timeseries\n",
      "---\n",
      "Name: accelerate\n",
      "Version: 0.21.0\n",
      "Summary: Accelerate\n",
      "Home-page: https://github.com/huggingface/accelerate\n",
      "Author: The HuggingFace team\n",
      "Author-email: sylvain@huggingface.co\n",
      "License: Apache\n",
      "Location: /opt/conda/lib/python3.11/site-packages\n",
      "Requires: numpy, packaging, psutil, pyyaml, torch\n",
      "Required-by: autogluon.multimodal, autogluon.timeseries\n"
     ]
    }
   ],
   "source": [
    "!pip show transformers accelerate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "cac5e0ef-fbfb-4d5e-83e4-a4bb708081c3",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: transformers in /opt/conda/lib/python3.11/site-packages (4.38.2)\n",
      "Collecting transformers\n",
      "  Using cached transformers-4.46.2-py3-none-any.whl.metadata (44 kB)\n",
      "Requirement already satisfied: accelerate in /opt/conda/lib/python3.11/site-packages (0.21.0)\n",
      "Collecting accelerate\n",
      "  Using cached accelerate-1.1.1-py3-none-any.whl.metadata (19 kB)\n",
      "Requirement already satisfied: filelock in /opt/conda/lib/python3.11/site-packages (from transformers) (3.16.1)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.23.2 in /opt/conda/lib/python3.11/site-packages (from transformers) (0.25.2)\n",
      "Requirement already satisfied: numpy>=1.17 in /opt/conda/lib/python3.11/site-packages (from transformers) (1.26.4)\n",
      "Requirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.11/site-packages (from transformers) (24.1)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /opt/conda/lib/python3.11/site-packages (from transformers) (6.0.2)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /opt/conda/lib/python3.11/site-packages (from transformers) (2024.9.11)\n",
      "Requirement already satisfied: requests in /opt/conda/lib/python3.11/site-packages (from transformers) (2.32.3)\n",
      "Requirement already satisfied: safetensors>=0.4.1 in /opt/conda/lib/python3.11/site-packages (from transformers) (0.4.5)\n",
      "Collecting tokenizers<0.21,>=0.20 (from transformers)\n",
      "  Using cached tokenizers-0.20.3-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.7 kB)\n",
      "Requirement already satisfied: tqdm>=4.27 in /opt/conda/lib/python3.11/site-packages (from transformers) (4.66.5)\n",
      "Requirement already satisfied: psutil in /opt/conda/lib/python3.11/site-packages (from accelerate) (5.9.8)\n",
      "Requirement already satisfied: torch>=1.10.0 in /opt/conda/lib/python3.11/site-packages (from accelerate) (2.3.1.post300)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /opt/conda/lib/python3.11/site-packages (from huggingface-hub<1.0,>=0.23.2->transformers) (2023.6.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /opt/conda/lib/python3.11/site-packages (from huggingface-hub<1.0,>=0.23.2->transformers) (4.12.2)\n",
      "Requirement already satisfied: sympy in /opt/conda/lib/python3.11/site-packages (from torch>=1.10.0->accelerate) (1.13.3)\n",
      "Requirement already satisfied: networkx in /opt/conda/lib/python3.11/site-packages (from torch>=1.10.0->accelerate) (3.4.1)\n",
      "Requirement already satisfied: jinja2 in /opt/conda/lib/python3.11/site-packages (from torch>=1.10.0->accelerate) (3.1.4)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.11/site-packages (from requests->transformers) (3.4.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.11/site-packages (from requests->transformers) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.11/site-packages (from requests->transformers) (1.26.19)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.11/site-packages (from requests->transformers) (2024.8.30)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.11/site-packages (from jinja2->torch>=1.10.0->accelerate) (3.0.2)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /opt/conda/lib/python3.11/site-packages (from sympy->torch>=1.10.0->accelerate) (1.3.0)\n",
      "Using cached transformers-4.46.2-py3-none-any.whl (10.0 MB)\n",
      "Using cached accelerate-1.1.1-py3-none-any.whl (333 kB)\n",
      "Using cached tokenizers-0.20.3-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.0 MB)\n",
      "Installing collected packages: tokenizers, accelerate, transformers\n",
      "  Attempting uninstall: tokenizers\n",
      "    Found existing installation: tokenizers 0.15.2\n",
      "    Uninstalling tokenizers-0.15.2:\n",
      "      Successfully uninstalled tokenizers-0.15.2\n",
      "  Attempting uninstall: accelerate\n",
      "    Found existing installation: accelerate 0.21.0\n",
      "    Uninstalling accelerate-0.21.0:\n",
      "      Successfully uninstalled accelerate-0.21.0\n",
      "  Attempting uninstall: transformers\n",
      "    Found existing installation: transformers 4.38.2\n",
      "    Uninstalling transformers-4.38.2:\n",
      "      Successfully uninstalled transformers-4.38.2\n",
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "autogluon-multimodal 1.1.1 requires nvidia-ml-py3==7.352.0, which is not installed.\n",
      "autogluon-multimodal 1.1.1 requires accelerate<0.22.0,>=0.21.0, but you have accelerate 1.1.1 which is incompatible.\n",
      "autogluon-multimodal 1.1.1 requires jsonschema<4.22,>=4.18, but you have jsonschema 4.23.0 which is incompatible.\n",
      "autogluon-multimodal 1.1.1 requires omegaconf<2.3.0,>=2.1.1, but you have omegaconf 2.3.0 which is incompatible.\n",
      "autogluon-multimodal 1.1.1 requires scikit-learn<1.4.1,>=1.3.0, but you have scikit-learn 1.5.2 which is incompatible.\n",
      "autogluon-multimodal 1.1.1 requires scipy<1.13,>=1.5.4, but you have scipy 1.14.1 which is incompatible.\n",
      "autogluon-multimodal 1.1.1 requires transformers[sentencepiece]<4.41.0,>=4.38.0, but you have transformers 4.46.2 which is incompatible.\n",
      "autogluon-timeseries 1.1.1 requires accelerate<0.22.0,>=0.21.0, but you have accelerate 1.1.1 which is incompatible.\n",
      "autogluon-timeseries 1.1.1 requires gluonts==0.15.1, but you have gluonts 0.14.3 which is incompatible.\n",
      "autogluon-timeseries 1.1.1 requires scipy<1.13,>=1.5.4, but you have scipy 1.14.1 which is incompatible.\n",
      "autogluon-timeseries 1.1.1 requires transformers[sentencepiece]<4.41.0,>=4.38.0, but you have transformers 4.46.2 which is incompatible.\u001b[0m\u001b[31m\n",
      "\u001b[0mSuccessfully installed accelerate-1.1.1 tokenizers-0.20.3 transformers-4.46.2\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install --upgrade transformers accelerate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b54f889d-4d77-4f47-a713-470969315649",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-11-17 06:17:22.830783: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2024-11-17 06:17:22.845426: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:485] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2024-11-17 06:17:22.863900: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:8454] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2024-11-17 06:17:22.869595: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1452] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2024-11-17 06:17:22.883158: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: SSE4.1 SSE4.2 AVX AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import datasets\n",
    "from datasets import Dataset\n",
    "from copy import deepcopy\n",
    "import time\n",
    "from dataclasses import dataclass\n",
    "from typing import Dict, List, Optional, Tuple\n",
    "from datasets import load_dataset\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from accelerate import Accelerator\n",
    "import nltk\n",
    "from nltk.corpus import wordnet\n",
    "from sklearn.metrics import accuracy_score, f1_score\n",
    "from transformers import (\n",
    "    AutoTokenizer, \n",
    "    AutoImageProcessor,  \n",
    "    AutoModel,\n",
    "    TrainingArguments, \n",
    "    Trainer,\n",
    "    logging\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "29a493f2-f0c2-4002-8afd-0190c4fb1e6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read the answer space\n",
    "with open(os.path.join(\"dataset\", \"answer_space.txt\")) as f:\n",
    "    answer_space = f.read().splitlines()\n",
    "\n",
    "# Read the files and process the answer column\n",
    "train_df = pd.read_csv(os.path.join(\"dataset\", \"data_train.csv\"))\n",
    "test_df = pd.read_csv(os.path.join(\"dataset\", \"data_eval.csv\"))\n",
    "\n",
    "# Function to get first answer and its index\n",
    "def process_answer(answer):\n",
    "    first_answer = answer.replace(\" \", \"\").split(\",\")[0]\n",
    "    return answer_space.index(first_answer)\n",
    "\n",
    "# Apply the processing to both dataframes\n",
    "train_df['label'] = train_df['answer'].apply(process_answer)\n",
    "test_df['label'] = test_df['answer'].apply(process_answer)\n",
    "\n",
    "# Convert to datasets format\n",
    "original_dataset = datasets.DatasetDict({\n",
    "    'train': Dataset.from_pandas(train_df),\n",
    "    'test': Dataset.from_pandas(test_df)\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "dad477ca-b814-40f9-8dec-f39552fb5dc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = original_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "7eb31562-2715-4bef-834d-31f581eafa3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_rows_train = 8*100\n",
    "n_rows_test = 2*100\n",
    "\n",
    "# Select the first n rows from each set\n",
    "train_sample = original_dataset['train'].select(range(n_rows_train))\n",
    "test_sample = original_dataset['test'].select(range(n_rows_test))\n",
    "\n",
    "# Create a smaller DatasetDict with the samples\n",
    "small_dataset = {\n",
    "    \"train\": train_sample,\n",
    "    \"test\": test_sample\n",
    "}\n",
    "\n",
    "dataset = small_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "477665a8-3e7a-4f91-89f7-3aabdf7d76c2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to /home/sagemaker-\n",
      "[nltk_data]     user/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device\n",
      "cuda:0\n"
     ]
    }
   ],
   "source": [
    "nltk.download('wordnet')\n",
    "# Disable MLflow logging\n",
    "os.environ['DISABLE_MLFLOW_INTEGRATION'] = 'TRUE'\n",
    "\n",
    "\n",
    "class MultimodalVQAModel(nn.Module):\n",
    "    def __init__(\n",
    "            self,\n",
    "            num_labels: int = len(answer_space),\n",
    "            intermediate_dim: int = 512,\n",
    "            pretrained_text_name: str = 'bert-base-uncased',\n",
    "            pretrained_image_name: str = 'google/vit-base-patch16-224-in21k'):\n",
    "\n",
    "        super(MultimodalVQAModel, self).__init__()\n",
    "        self.num_labels = num_labels\n",
    "        self.pretrained_text_name = pretrained_text_name\n",
    "        self.pretrained_image_name = pretrained_image_name\n",
    "\n",
    "        self.text_encoder = AutoModel.from_pretrained(\n",
    "            self.pretrained_text_name,\n",
    "            force_download=True\n",
    "        )\n",
    "        self.image_encoder = AutoModel.from_pretrained(\n",
    "            self.pretrained_image_name,\n",
    "            force_download=True\n",
    "        )\n",
    "        self.fusion = nn.Sequential(\n",
    "            nn.Linear(self.text_encoder.config.hidden_size + self.image_encoder.config.hidden_size, intermediate_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.5),\n",
    "        )\n",
    "\n",
    "        self.classifier = nn.Linear(intermediate_dim, self.num_labels)\n",
    "        self.criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "    def forward(\n",
    "            self,\n",
    "            input_ids: torch.LongTensor,\n",
    "            pixel_values: torch.FloatTensor,\n",
    "            attention_mask: Optional[torch.LongTensor] = None,\n",
    "            token_type_ids: Optional[torch.LongTensor] = None,\n",
    "            labels: Optional[torch.LongTensor] = None):\n",
    "\n",
    "        encoded_text = self.text_encoder(\n",
    "            input_ids=input_ids,\n",
    "            attention_mask=attention_mask,\n",
    "            token_type_ids=token_type_ids,\n",
    "            return_dict=True,\n",
    "        )\n",
    "        encoded_image = self.image_encoder(\n",
    "            pixel_values=pixel_values,\n",
    "            return_dict=True,\n",
    "        )\n",
    "        fused_output = self.fusion(\n",
    "            torch.cat(\n",
    "                [\n",
    "                    encoded_text['pooler_output'],\n",
    "                    encoded_image['pooler_output'],\n",
    "                ],\n",
    "                dim=1\n",
    "            )\n",
    "        )\n",
    "        logits = self.classifier(fused_output)\n",
    "\n",
    "        out = {\n",
    "            \"logits\": logits\n",
    "        }\n",
    "        if labels is not None:\n",
    "            loss = self.criterion(logits, labels)\n",
    "            out[\"loss\"] = loss\n",
    "\n",
    "        return out\n",
    "\n",
    "@dataclass\n",
    "class MultimodalCollator:\n",
    "    tokenizer: AutoTokenizer\n",
    "    preprocessor: AutoImageProcessor\n",
    "\n",
    "    def tokenize_text(self, texts: List[str]):\n",
    "        encoded_text = self.tokenizer(\n",
    "            text=texts,\n",
    "            padding='longest',\n",
    "            max_length=24,\n",
    "            truncation=True,\n",
    "            return_tensors='pt',\n",
    "            return_token_type_ids=True,\n",
    "            return_attention_mask=True,\n",
    "        )\n",
    "        return {\n",
    "            \"input_ids\": encoded_text['input_ids'].squeeze(),\n",
    "            \"token_type_ids\": encoded_text['token_type_ids'].squeeze(),\n",
    "            \"attention_mask\": encoded_text['attention_mask'].squeeze(),\n",
    "        }\n",
    "\n",
    "    def preprocess_images(self, images: List[str]):\n",
    "        processed_images = self.preprocessor(\n",
    "            images=[Image.open(os.path.join(\"dataset\", \"images\", image_id + \".png\")).convert('RGB') for image_id in images],\n",
    "            return_tensors=\"pt\",\n",
    "        )\n",
    "        return {\n",
    "            \"pixel_values\": processed_images['pixel_values'].squeeze(),\n",
    "        }\n",
    "\n",
    "    def __call__(self, raw_batch_dict):\n",
    "        return {\n",
    "            **self.tokenize_text(\n",
    "                raw_batch_dict['question']\n",
    "                if isinstance(raw_batch_dict, dict) else\n",
    "                [i['question'] for i in raw_batch_dict]\n",
    "            ),\n",
    "            **self.preprocess_images(\n",
    "                raw_batch_dict['image_id']\n",
    "                if isinstance(raw_batch_dict, dict) else\n",
    "                [i['image_id'] for i in raw_batch_dict]\n",
    "            ),\n",
    "            'labels': torch.tensor(\n",
    "                raw_batch_dict['label']\n",
    "                if isinstance(raw_batch_dict, dict) else\n",
    "                [i['label'] for i in raw_batch_dict],\n",
    "                dtype=torch.int64\n",
    "            ),\n",
    "        }\n",
    "\n",
    "def wup_measure(a,b,similarity_threshold=0.925):\n",
    "    \"\"\"\n",
    "    Returns Wu-Palmer similarity score.\n",
    "    More specifically, it computes:\n",
    "        max_{x \\in interp(a)} max_{y \\in interp(b)} wup(x,y)\n",
    "        where interp is a 'interpretation field'\n",
    "    \"\"\"\n",
    "    def get_semantic_field(a):\n",
    "        weight = 1.0\n",
    "        semantic_field = wordnet.synsets(a,pos=wordnet.NOUN)\n",
    "        return (semantic_field,weight)\n",
    "\n",
    "\n",
    "    def get_stem_word(a):\n",
    "        \"\"\"\n",
    "        Sometimes answer has form word\\d+:wordid.\n",
    "        If so we return word and downweight\n",
    "        \"\"\"\n",
    "        weight = 1.0\n",
    "        return (a,weight)\n",
    "\n",
    "\n",
    "    global_weight=1.0\n",
    "\n",
    "    (a,global_weight_a)=get_stem_word(a)\n",
    "    (b,global_weight_b)=get_stem_word(b)\n",
    "    global_weight = min(global_weight_a,global_weight_b)\n",
    "\n",
    "    if a==b:\n",
    "        # they are the same\n",
    "        return 1.0*global_weight\n",
    "\n",
    "    if a==[] or b==[]:\n",
    "        return 0\n",
    "\n",
    "\n",
    "    interp_a,weight_a = get_semantic_field(a)\n",
    "    interp_b,weight_b = get_semantic_field(b)\n",
    "\n",
    "    if interp_a == [] or interp_b == []:\n",
    "        return 0\n",
    "\n",
    "    # we take the most optimistic interpretation\n",
    "    global_max=0.0\n",
    "    for x in interp_a:\n",
    "        for y in interp_b:\n",
    "            local_score=x.wup_similarity(y)\n",
    "            if local_score > global_max:\n",
    "                global_max=local_score\n",
    "\n",
    "    # we need to use the semantic fields and therefore we downweight\n",
    "    # unless the score is high which indicates both are synonyms\n",
    "    if global_max < similarity_threshold:\n",
    "        interp_weight = 0.1\n",
    "    else:\n",
    "        interp_weight = 1.0\n",
    "\n",
    "    final_score=global_max*weight_a*weight_b*interp_weight*global_weight\n",
    "    return final_score\n",
    "\n",
    "def batch_wup_measure(labels, preds):\n",
    "    wup_scores = [wup_measure(answer_space[label], answer_space[pred]) for label, pred in zip(labels, preds)]\n",
    "    return np.mean(wup_scores)\n",
    "\n",
    "def compute_metrics(eval_tuple: Tuple[np.ndarray, np.ndarray]) -> Dict[str, float]:\n",
    "    logits, labels = eval_tuple\n",
    "    preds = logits.argmax(axis=-1)\n",
    "    return {\n",
    "        \"wups\": batch_wup_measure(labels, preds),\n",
    "        \"acc\": accuracy_score(labels, preds),\n",
    "        \"f1\": f1_score(labels, preds, average='macro')\n",
    "    }\n",
    "\n",
    "def createMultimodalVQACollatorAndModel(text='bert-base-uncased', image='google/vit-base-patch16-224-in21k'):\n",
    "    tokenizer = AutoTokenizer.from_pretrained(text, force_download=True)\n",
    "    preprocessor = AutoImageProcessor.from_pretrained(image, force_download=True, use_fast=True )\n",
    "\n",
    "    multi_collator = MultimodalCollator(\n",
    "        tokenizer=tokenizer,\n",
    "        preprocessor=preprocessor,\n",
    "    )\n",
    "\n",
    "    multi_model = MultimodalVQAModel(pretrained_text_name=text, pretrained_image_name=image).to(device)\n",
    "    return multi_collator, multi_model\n",
    "\n",
    "def createAndTrainModel(dataset, args, text_model='bert-base-uncased',\n",
    "                       image_model='google/vit-base-patch16-224-in21k',\n",
    "                       multimodal_model='bert_vit'):\n",
    "    # Create output directory with timestamp to ensure uniqueness\n",
    "    timestamp = time.strftime(\"%Y%m%d-%H%M%S\")\n",
    "    output_dir = os.path.join(\"checkpoint\", multimodal_model, f\"run_{timestamp}\")\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "    collator, model = createMultimodalVQACollatorAndModel(text_model, image_model)\n",
    "\n",
    "    # Create a new TrainingArguments object with unique run name\n",
    "    training_args = TrainingArguments(\n",
    "        output_dir=output_dir,\n",
    "        run_name=f\"run_{timestamp}\",\n",
    "        evaluation_strategy=\"epoch\",\n",
    "        save_strategy=\"epoch\",\n",
    "        load_best_model_at_end=True,\n",
    "        metric_for_best_model=\"wups\",\n",
    "        greater_is_better=True,\n",
    "        per_device_train_batch_size=args.per_device_train_batch_size if hasattr(args, 'per_device_train_batch_size') else 8,\n",
    "        per_device_eval_batch_size=args.per_device_eval_batch_size if hasattr(args, 'per_device_eval_batch_size') else 8,\n",
    "        num_train_epochs=args.num_train_epochs if hasattr(args, 'num_train_epochs') else 3,\n",
    "        learning_rate=args.learning_rate if hasattr(args, 'learning_rate') else 5e-5,\n",
    "        report_to=[],\n",
    "        remove_unused_columns=False,  # Important: Keep all columns\n",
    "    )\n",
    "\n",
    "    # Verify dataset format\n",
    "    print(\"Sample training example:\", dataset['train'][0])\n",
    "    print(\"Sample test example:\", dataset['test'][0])\n",
    "\n",
    "    multi_trainer = Trainer(\n",
    "        model=model,\n",
    "        args=training_args,\n",
    "        train_dataset=dataset['train'],\n",
    "        eval_dataset=dataset['test'],\n",
    "        data_collator=collator,\n",
    "        compute_metrics=compute_metrics,\n",
    "    )\n",
    "\n",
    "    train_multi_metrics = multi_trainer.train()\n",
    "    eval_multi_metrics = multi_trainer.evaluate()\n",
    "\n",
    "    # Save the final model explicitly\n",
    "    model_save_path = os.path.join(output_dir, \"final_model.pt\")\n",
    "    torch.save({\n",
    "        'model_state_dict': model.state_dict(),\n",
    "        'train_metrics': train_multi_metrics,\n",
    "        'eval_metrics': eval_multi_metrics\n",
    "    }, model_save_path)\n",
    "\n",
    "    return collator, model, train_multi_metrics, eval_multi_metrics\n",
    "\n",
    "# Set device\n",
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "print(\"Device\")\n",
    "print(device)\n",
    "\n",
    "# Training arguments\n",
    "args = TrainingArguments(\n",
    "    output_dir=\"checkpoint\",\n",
    "    seed=42,\n",
    "    eval_strategy=\"steps\",\n",
    "    eval_steps=100,\n",
    "    logging_strategy=\"steps\",\n",
    "    logging_steps=100,\n",
    "    save_strategy=\"steps\",\n",
    "    save_steps=100,\n",
    "    save_total_limit=3,\n",
    "    metric_for_best_model='wups',\n",
    "    per_device_train_batch_size=16,\n",
    "    per_device_eval_batch_size=16,\n",
    "    remove_unused_columns=False,\n",
    "    num_train_epochs=5,\n",
    "    fp16=True,\n",
    "    dataloader_num_workers=8,\n",
    "    load_best_model_at_end=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81dfefc0-1829-4be7-9fcf-ce60cc1083cd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6b5d31697fa6456d89759087aa530ac0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/48.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "613ad74fc3004d949c0c9ed7fa7bb2a7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/570 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7b1c4bf81cb44094928d8e159cae8646",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/48.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e91cb74e961d401c859e9a9942376481",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d6a21fd40d15480184ad71595b9258c9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/466k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0903409ab9a741068209de3390442fe4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/48.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "190bc95552c7470d99701bc95f698c18",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "preprocessor_config.json:   0%|          | 0.00/160 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b6f03ca1859048b8a1409543d0faa0e2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/502 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7f9c0bad823e4b1e95ac148afc3d11de",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "preprocessor_config.json:   0%|          | 0.00/160 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "084943f91a674801a4bb0132762a597c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/570 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c9ae2e819cfc49faa95289c1e94916d0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/570 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f0fb085fc72d497eb4caf85bea2980e1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/440M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "643252ee25d54258a4df223afcf68800",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/502 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c0ef485a077c47639b39d4082d14b4db",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/502 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "325f5eb720644b3a9d0d7ec1f524a7e0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/346M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.11/site-packages/transformers/training_args.py:1568: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of ðŸ¤— Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample training example: {'question': 'what is the object on the shelves', 'answer': 'cup', 'image_id': 'image100', 'label': 149}\n",
      "Sample test example: {'question': 'what is the colour of the bag on the chair', 'answer': 'pink', 'image_id': 'image399', 'label': 387}\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='78' max='3120' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [  78/3120 00:30 < 20:35, 2.46 it/s, Epoch 0.12/5]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "collator, model, train_multi_metrics, eval_multi_metrics = createAndTrainModel(dataset, args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "019cb495-e729-41bf-94fe-24cc2d2451a4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['ceiling', 'shelves', 'head_phones', 'chair', 'box_of_paper']\n",
      "['air_conditioner', 'clothing_hamper', 'stamp', 'fire_extinguisher', 'medal']\n",
      "Predictions vs Labels:  0.02608187134502924\n",
      "Labels vs Labels:  1.0\n"
     ]
    }
   ],
   "source": [
    "labels = np.random.randint(len(answer_space), size=5)\n",
    "preds = np.random.randint(len(answer_space), size=5)\n",
    "\n",
    "def showAnswers(ids):\n",
    "    print([answer_space[id] for id in ids])\n",
    "\n",
    "showAnswers(labels)\n",
    "showAnswers(preds)\n",
    "\n",
    "print(\"Predictions vs Labels: \", batch_wup_measure(labels, preds))\n",
    "print(\"Labels vs Labels: \", batch_wup_measure(labels, labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "bf18b418-1fce-46ea-918b-de64033e0bdb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'eval_loss': 3.4556491374969482,\n",
       " 'eval_wups': 0.3071833106722724,\n",
       " 'eval_acc': 0.2610264635124298,\n",
       " 'eval_f1': 0.03549650549105638,\n",
       " 'eval_runtime': 39.9818,\n",
       " 'eval_samples_per_second': 62.378,\n",
       " 'eval_steps_per_second': 0.975,\n",
       " 'epoch': 10.0}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "eval_multi_metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed14ec7f-cf15-4889-817a-5de2b427022e",
   "metadata": {},
   "source": [
    "# Load and test model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2bc019bc-30f8-494d-b6cf-ecee8673428e",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = MultimodalVQAModel()\n",
    "# RTC:checkpoint/bert_vit/run_20241116-153943/final_model.pt\n",
    "checkpoint_path = os.path.join(\"checkpoint\", \"bert_vit\", \"final\", \"full_64_64_10.pt\")\n",
    "if os.path.exists(checkpoint_path):\n",
    "    checkpoint = torch.load(checkpoint_path)\n",
    "    model.load_state_dict(checkpoint['model_state_dict'])\n",
    "    model.to(device)\n",
    "    print(\"Model loaded successfully\")\n",
    "    print(\"Final evaluation metrics:\", checkpoint['eval_metrics'])\n",
    "else:\n",
    "    print(\"No saved model found at:\", checkpoint_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5dbd01f4-d416-46c6-90bb-0a6292c25ad9",
   "metadata": {},
   "outputs": [],
   "source": [
    "sample = collator(dataset[\"test\"][100:1])\n",
    "\n",
    "input_ids = sample[\"input_ids\"].to(device)\n",
    "token_type_ids = sample[\"token_type_ids\"].to(device)\n",
    "attention_mask = sample[\"attention_mask\"].to(device)\n",
    "pixel_values = sample[\"pixel_values\"].to(device)\n",
    "labels = sample[\"labels\"].to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84c2f39e-e29d-437b-a8fc-d4df9cfd6b5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.eval()\n",
    "output = model(input_ids, pixel_values, attention_mask, token_type_ids, labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31d47dcf-3fbc-4c54-8c69-f2337c4ef4bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "preds = output[\"logits\"].argmax(axis=-1).cpu().numpy()\n",
    "preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "703ef3fe-fc27-4541-8dcd-1ff1cc77efdc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import display\n",
    "\n",
    "def showExample(train=True, id=None):\n",
    "    if train:\n",
    "        data = dataset[\"train\"]\n",
    "    else:\n",
    "        data = dataset[\"test\"]\n",
    "    if id == None:\n",
    "        id = np.random.randint(len(data))\n",
    "    image = Image.open(os.path.join(\"dataset\", \"images\", data[id][\"image_id\"] + \".png\"))\n",
    "    display(image)\n",
    "\n",
    "    print(\"Question:\\t\", data[id][\"question\"])\n",
    "    print(\"Answer:\\t\\t\", data[id][\"answer\"], \"(Label: {0})\".format(data[id][\"label\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5eefccf9-13f9-40eb-8f78-7258cb879780",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(000, 005):\n",
    "    print(\"*********************************************************\")\n",
    "    showExample(train=False, id=i)\n",
    "    print(\"Predicted Answer:\\t\", answer_space[preds[i-1000]])\n",
    "    print(\"*********************************************************\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8047953-2c9b-47bf-9cff-0397b8a916a3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
