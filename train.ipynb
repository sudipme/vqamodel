{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b22da12a-0fb4-4fdf-9441-b0ec22bfb2a9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Name: transformers\n",
      "Version: 4.46.2\n",
      "Summary: State-of-the-art Machine Learning for JAX, PyTorch and TensorFlow\n",
      "Home-page: https://github.com/huggingface/transformers\n",
      "Author: The Hugging Face team (past and future) with the help of all our contributors (https://github.com/huggingface/transformers/graphs/contributors)\n",
      "Author-email: transformers@huggingface.co\n",
      "License: Apache 2.0 License\n",
      "Location: /opt/conda/lib/python3.11/site-packages\n",
      "Requires: filelock, huggingface-hub, numpy, packaging, pyyaml, regex, requests, safetensors, tokenizers, tqdm\n",
      "Required-by: autogluon.multimodal, autogluon.timeseries\n"
     ]
    }
   ],
   "source": [
    "!pip show transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f50c76ce-bc6c-490e-8f08-391eed08a859",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-11-17 12:31:55.298103: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:485] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2024-11-17 12:31:55.311012: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:8454] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2024-11-17 12:31:55.315015: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1452] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2024-11-17 12:31:55.325050: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: SSE4.1 SSE4.2 AVX AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "[nltk_data] Downloading package wordnet to /home/sagemaker-\n",
      "[nltk_data]     user/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import datasets\n",
    "from datasets import Dataset\n",
    "from copy import deepcopy\n",
    "import time\n",
    "from dataclasses import dataclass\n",
    "from typing import Dict, List, Optional, Tuple\n",
    "from datasets import load_dataset\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from accelerate import Accelerator\n",
    "import nltk\n",
    "from nltk.corpus import wordnet\n",
    "from sklearn.metrics import accuracy_score, f1_score\n",
    "from transformers import (\n",
    "    AutoTokenizer, \n",
    "    AutoImageProcessor,  \n",
    "    AutoModel,\n",
    "    TrainingArguments, \n",
    "    Trainer,\n",
    "    logging\n",
    ")\n",
    "\n",
    "nltk.download('wordnet')\n",
    "# Disable MLflow logging\n",
    "os.environ['DISABLE_MLFLOW_INTEGRATION'] = 'TRUE'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ec302a48-7526-4ed9-a1ff-98c430b46182",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read the answer space\n",
    "with open(os.path.join(\"dataset\", \"answer_space.txt\")) as f:\n",
    "    answer_space = f.read().splitlines()\n",
    "\n",
    "# Read the files and process the answer column\n",
    "train_df = pd.read_csv(os.path.join(\"dataset\", \"data_train.csv\"))\n",
    "test_df = pd.read_csv(os.path.join(\"dataset\", \"data_eval.csv\"))\n",
    "\n",
    "# Function to get first answer and its index\n",
    "def process_answer(answer):\n",
    "    first_answer = answer.replace(\" \", \"\").split(\",\")[0]\n",
    "    return answer_space.index(first_answer)\n",
    "\n",
    "# Apply the processing to both dataframes\n",
    "train_df['label'] = train_df['answer'].apply(process_answer)\n",
    "test_df['label'] = test_df['answer'].apply(process_answer)\n",
    "\n",
    "# Convert to datasets format\n",
    "original_dataset = datasets.DatasetDict({\n",
    "    'train': Dataset.from_pandas(train_df),\n",
    "    'test': Dataset.from_pandas(test_df)\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "58ee2222-0cfa-47bc-9955-ab4fae02f8b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = original_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "db82947a-33aa-4566-8e30-2f353744d18d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device\n",
      "cuda:0\n"
     ]
    }
   ],
   "source": [
    "class MultimodalVQAModel(nn.Module):\n",
    "    def __init__(\n",
    "            self,\n",
    "            num_labels: int = len(answer_space),\n",
    "            intermediate_dim: int = 512,\n",
    "            pretrained_text_name: str = 'bert-large-uncased',\n",
    "            pretrained_image_name: str = 'google/vit-large-patch16-224-in21k'):\n",
    "\n",
    "        super(MultimodalVQAModel, self).__init__()\n",
    "        self.num_labels = num_labels\n",
    "        self.pretrained_text_name = pretrained_text_name\n",
    "        self.pretrained_image_name = pretrained_image_name\n",
    "\n",
    "        self.text_encoder = AutoModel.from_pretrained(\n",
    "            self.pretrained_text_name,\n",
    "            force_download=True\n",
    "        )\n",
    "        self.image_encoder = AutoModel.from_pretrained(\n",
    "            self.pretrained_image_name,\n",
    "            force_download=True\n",
    "        )\n",
    "        self.fusion = nn.Sequential(\n",
    "            nn.Linear(self.text_encoder.config.hidden_size + self.image_encoder.config.hidden_size, intermediate_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.5),\n",
    "        )\n",
    "\n",
    "        self.classifier = nn.Linear(intermediate_dim, self.num_labels)\n",
    "        self.criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "    def forward(\n",
    "            self,\n",
    "            input_ids: torch.LongTensor,\n",
    "            pixel_values: torch.FloatTensor,\n",
    "            attention_mask: Optional[torch.LongTensor] = None,\n",
    "            token_type_ids: Optional[torch.LongTensor] = None,\n",
    "            labels: Optional[torch.LongTensor] = None):\n",
    "\n",
    "        encoded_text = self.text_encoder(\n",
    "            input_ids=input_ids,\n",
    "            attention_mask=attention_mask,\n",
    "            token_type_ids=token_type_ids,\n",
    "            return_dict=True,\n",
    "        )\n",
    "        encoded_image = self.image_encoder(\n",
    "            pixel_values=pixel_values,\n",
    "            return_dict=True,\n",
    "        )\n",
    "        fused_output = self.fusion(\n",
    "            torch.cat(\n",
    "                [\n",
    "                    encoded_text['pooler_output'],\n",
    "                    encoded_image['pooler_output'],\n",
    "                ],\n",
    "                dim=1\n",
    "            )\n",
    "        )\n",
    "        logits = self.classifier(fused_output)\n",
    "\n",
    "        out = {\n",
    "            \"logits\": logits\n",
    "        }\n",
    "        if labels is not None:\n",
    "            loss = self.criterion(logits, labels)\n",
    "            out[\"loss\"] = loss\n",
    "\n",
    "        return out\n",
    "\n",
    "@dataclass\n",
    "class MultimodalCollator:\n",
    "    tokenizer: AutoTokenizer\n",
    "    preprocessor: AutoImageProcessor\n",
    "\n",
    "    def tokenize_text(self, texts: List[str]):\n",
    "        encoded_text = self.tokenizer(\n",
    "            text=texts,\n",
    "            padding='longest',\n",
    "            max_length=24,\n",
    "            truncation=True,\n",
    "            return_tensors='pt',\n",
    "            return_token_type_ids=True,\n",
    "            return_attention_mask=True,\n",
    "        )\n",
    "        return {\n",
    "            \"input_ids\": encoded_text['input_ids'].squeeze(),\n",
    "            \"token_type_ids\": encoded_text['token_type_ids'].squeeze(),\n",
    "            \"attention_mask\": encoded_text['attention_mask'].squeeze(),\n",
    "        }\n",
    "\n",
    "    def preprocess_images(self, images: List[str]):\n",
    "        processed_images = self.preprocessor(\n",
    "            images=[Image.open(os.path.join(\"dataset\", \"images\", image_id + \".png\")).convert('RGB') for image_id in images],\n",
    "            return_tensors=\"pt\",\n",
    "        )\n",
    "        return {\n",
    "            \"pixel_values\": processed_images['pixel_values'].squeeze(),\n",
    "        }\n",
    "\n",
    "    def __call__(self, raw_batch_dict):\n",
    "        return {\n",
    "            **self.tokenize_text(\n",
    "                raw_batch_dict['question']\n",
    "                if isinstance(raw_batch_dict, dict) else\n",
    "                [i['question'] for i in raw_batch_dict]\n",
    "            ),\n",
    "            **self.preprocess_images(\n",
    "                raw_batch_dict['image_id']\n",
    "                if isinstance(raw_batch_dict, dict) else\n",
    "                [i['image_id'] for i in raw_batch_dict]\n",
    "            ),\n",
    "            'labels': torch.tensor(\n",
    "                raw_batch_dict['label']\n",
    "                if isinstance(raw_batch_dict, dict) else\n",
    "                [i['label'] for i in raw_batch_dict],\n",
    "                dtype=torch.int64\n",
    "            ),\n",
    "        }\n",
    "\n",
    "def wup_measure(a,b,similarity_threshold=0.925):\n",
    "    \"\"\"\n",
    "    Returns Wu-Palmer similarity score.\n",
    "    More specifically, it computes:\n",
    "        max_{x \\in interp(a)} max_{y \\in interp(b)} wup(x,y)\n",
    "        where interp is a 'interpretation field'\n",
    "    \"\"\"\n",
    "    def get_semantic_field(a):\n",
    "        weight = 1.0\n",
    "        semantic_field = wordnet.synsets(a,pos=wordnet.NOUN)\n",
    "        return (semantic_field,weight)\n",
    "\n",
    "\n",
    "    def get_stem_word(a):\n",
    "        \"\"\"\n",
    "        Sometimes answer has form word\\d+:wordid.\n",
    "        If so we return word and downweight\n",
    "        \"\"\"\n",
    "        weight = 1.0\n",
    "        return (a,weight)\n",
    "\n",
    "\n",
    "    global_weight=1.0\n",
    "\n",
    "    (a,global_weight_a)=get_stem_word(a)\n",
    "    (b,global_weight_b)=get_stem_word(b)\n",
    "    global_weight = min(global_weight_a,global_weight_b)\n",
    "\n",
    "    if a==b:\n",
    "        # they are the same\n",
    "        return 1.0*global_weight\n",
    "\n",
    "    if a==[] or b==[]:\n",
    "        return 0\n",
    "\n",
    "\n",
    "    interp_a,weight_a = get_semantic_field(a)\n",
    "    interp_b,weight_b = get_semantic_field(b)\n",
    "\n",
    "    if interp_a == [] or interp_b == []:\n",
    "        return 0\n",
    "\n",
    "    # we take the most optimistic interpretation\n",
    "    global_max=0.0\n",
    "    for x in interp_a:\n",
    "        for y in interp_b:\n",
    "            local_score=x.wup_similarity(y)\n",
    "            if local_score > global_max:\n",
    "                global_max=local_score\n",
    "\n",
    "    # we need to use the semantic fields and therefore we downweight\n",
    "    # unless the score is high which indicates both are synonyms\n",
    "    if global_max < similarity_threshold:\n",
    "        interp_weight = 0.1\n",
    "    else:\n",
    "        interp_weight = 1.0\n",
    "\n",
    "    final_score=global_max*weight_a*weight_b*interp_weight*global_weight\n",
    "    return final_score\n",
    "\n",
    "def batch_wup_measure(labels, preds):\n",
    "    wup_scores = [wup_measure(answer_space[label], answer_space[pred]) for label, pred in zip(labels, preds)]\n",
    "    return np.mean(wup_scores)\n",
    "\n",
    "def compute_metrics(eval_tuple: Tuple[np.ndarray, np.ndarray]) -> Dict[str, float]:\n",
    "    logits, labels = eval_tuple\n",
    "    preds = logits.argmax(axis=-1)\n",
    "    return {\n",
    "        \"wups\": batch_wup_measure(labels, preds),\n",
    "        \"acc\": accuracy_score(labels, preds),\n",
    "        \"f1\": f1_score(labels, preds, average='macro')\n",
    "    }\n",
    "\n",
    "def createMultimodalVQACollatorAndModel(text='bert-large-uncased', image='google/vit-large-patch16-224-in21k'):\n",
    "    tokenizer = AutoTokenizer.from_pretrained(text, force_download=True)\n",
    "    preprocessor = AutoImageProcessor.from_pretrained(image, force_download=True, use_fast=True )\n",
    "\n",
    "    multi_collator = MultimodalCollator(\n",
    "        tokenizer=tokenizer,\n",
    "        preprocessor=preprocessor,\n",
    "    )\n",
    "\n",
    "    multi_model = MultimodalVQAModel(pretrained_text_name=text, pretrained_image_name=image).to(device)\n",
    "    return multi_collator, multi_model\n",
    "\n",
    "def createAndTrainModel(dataset, args, text_model='bert-large-uncased',\n",
    "                       image_model='google/vit-large-patch16-224-in21k',\n",
    "                       multimodal_model='bert_vit'):\n",
    "    # Create output directory with timestamp to ensure uniqueness\n",
    "    timestamp = time.strftime(\"%Y%m%d-%H%M%S\")\n",
    "    output_dir = os.path.join(\"checkpoint\", multimodal_model, f\"run_{timestamp}\")\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "    collator, model = createMultimodalVQACollatorAndModel(text_model, image_model)\n",
    "\n",
    "    # Create a new TrainingArguments object with unique run name\n",
    "    training_args = TrainingArguments(\n",
    "        output_dir=output_dir,\n",
    "        run_name=f\"run_{timestamp}\",\n",
    "        evaluation_strategy=\"epoch\",\n",
    "        save_strategy=\"epoch\",\n",
    "        load_best_model_at_end=True,\n",
    "        metric_for_best_model=\"wups\",\n",
    "        greater_is_better=True,\n",
    "        per_device_train_batch_size=args.per_device_train_batch_size if hasattr(args, 'per_device_train_batch_size') else 8,\n",
    "        per_device_eval_batch_size=args.per_device_eval_batch_size if hasattr(args, 'per_device_eval_batch_size') else 8,\n",
    "        num_train_epochs=args.num_train_epochs if hasattr(args, 'num_train_epochs') else 3,\n",
    "        learning_rate=args.learning_rate if hasattr(args, 'learning_rate') else 5e-5,\n",
    "        report_to=[],\n",
    "        remove_unused_columns=False,  # Important: Keep all columns\n",
    "    )\n",
    "\n",
    "    # Verify dataset format\n",
    "    print(\"Sample training example:\", dataset['train'][0])\n",
    "    print(\"Sample test example:\", dataset['test'][0])\n",
    "\n",
    "    multi_trainer = Trainer(\n",
    "        model=model,\n",
    "        args=training_args,\n",
    "        train_dataset=dataset['train'],\n",
    "        eval_dataset=dataset['test'],\n",
    "        data_collator=collator,\n",
    "        compute_metrics=compute_metrics,\n",
    "    )\n",
    "\n",
    "    train_multi_metrics = multi_trainer.train()\n",
    "    eval_multi_metrics = multi_trainer.evaluate()\n",
    "\n",
    "    # Save the final model explicitly\n",
    "    model_save_path = os.path.join(output_dir, \"final_model.pt\")\n",
    "    torch.save({\n",
    "        'model_state_dict': model.state_dict(),\n",
    "        'train_metrics': train_multi_metrics,\n",
    "        'eval_metrics': eval_multi_metrics\n",
    "    }, model_save_path)\n",
    "\n",
    "    return collator, model, train_multi_metrics, eval_multi_metrics\n",
    "\n",
    "# Set device\n",
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "print(\"Device\")\n",
    "print(device)\n",
    "\n",
    "# Training arguments\n",
    "args = TrainingArguments(\n",
    "    output_dir=\"checkpoint\",\n",
    "    seed=42,\n",
    "    eval_strategy=\"steps\",\n",
    "    eval_steps=100,\n",
    "    logging_strategy=\"steps\",\n",
    "    logging_steps=100,\n",
    "    save_strategy=\"steps\",\n",
    "    save_steps=100,\n",
    "    save_total_limit=3,\n",
    "    metric_for_best_model='wups',\n",
    "    per_device_train_batch_size=16,\n",
    "    per_device_eval_batch_size=16,\n",
    "    remove_unused_columns=False,\n",
    "    num_train_epochs=10,\n",
    "    fp16=True,\n",
    "    dataloader_num_workers=8,\n",
    "    load_best_model_at_end=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac856a8a-f0fb-4492-8a0a-96b3fd08ff8e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1ed961357c8f4f4980ce2388cdec78d2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/48.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b3b53e4b1f6e423c81af5d0d5fc0b900",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/571 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "33e67166a0804b3fbd1da32819f620d5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/48.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "11dc859ec7414324b6a52aa958303f92",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e7473ade108445ef96979c605b505634",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/466k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "578a7adb1b224cc6a45fcb621b3de2a8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/48.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.11/site-packages/transformers/training_args.py:1568: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of ðŸ¤— Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample training example: {'question': 'what is the object on the shelves', 'answer': 'cup', 'image_id': 'image100', 'label': 149}\n",
      "Sample test example: {'question': 'what is the colour of the bag on the chair', 'answer': 'pink', 'image_id': 'image399', 'label': 387}\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1978' max='6240' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [1978/6240 22:57 < 49:31, 1.43 it/s, Epoch 3.17/10]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Wups</th>\n",
       "      <th>Acc</th>\n",
       "      <th>F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>5.340800</td>\n",
       "      <td>4.865862</td>\n",
       "      <td>0.085693</td>\n",
       "      <td>0.053328</td>\n",
       "      <td>0.001751</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>4.900700</td>\n",
       "      <td>4.724402</td>\n",
       "      <td>0.116270</td>\n",
       "      <td>0.072173</td>\n",
       "      <td>0.005169</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>4.733400</td>\n",
       "      <td>4.647911</td>\n",
       "      <td>0.117333</td>\n",
       "      <td>0.076183</td>\n",
       "      <td>0.005938</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "collator, model, train_multi_metrics, eval_multi_metrics = createAndTrainModel(dataset, args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ad61acea-0e69-46ea-a00e-69899e804c61",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip freeze > final.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1161f4e3-4140-49d6-9d19-5bc57816d7ef",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
