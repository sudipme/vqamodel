{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b54f889d-4d77-4f47-a713-470969315649",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import datasets\n",
    "from datasets import Dataset\n",
    "from copy import deepcopy\n",
    "import time\n",
    "from dataclasses import dataclass\n",
    "from typing import Dict, List, Optional, Tuple\n",
    "from datasets import load_dataset\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from accelerate import Accelerator\n",
    "import nltk\n",
    "from nltk.corpus import wordnet\n",
    "from sklearn.metrics import accuracy_score, f1_score\n",
    "from transformers import (\n",
    "    AutoTokenizer, \n",
    "    AutoImageProcessor,  \n",
    "    AutoModel,\n",
    "    TrainingArguments, \n",
    "    Trainer,\n",
    "    logging\n",
    ")\n",
    "from transformers import Trainer, TrainingArguments\n",
    "from transformers.trainer_utils import TrainOutput"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96b480ae-3c20-4cb6-a574-b55474f0d357",
   "metadata": {},
   "source": [
    "# Load dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29a493f2-f0c2-4002-8afd-0190c4fb1e6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read the answer space\n",
    "with open(os.path.join(\"dataset\", \"answer_space.txt\")) as f:\n",
    "    answer_space = f.read().splitlines()\n",
    "\n",
    "# Read the files and process the answer column\n",
    "train_df = pd.read_csv(os.path.join(\"dataset\", \"data_train.csv\"))\n",
    "test_df = pd.read_csv(os.path.join(\"dataset\", \"data_eval.csv\"))\n",
    "\n",
    "# Function to get first answer and its index\n",
    "def process_answer(answer):\n",
    "    first_answer = answer.replace(\" \", \"\").split(\",\")[0]\n",
    "    return answer_space.index(first_answer)\n",
    "\n",
    "# Apply the processing to both dataframes\n",
    "train_df['label'] = train_df['answer'].apply(process_answer)\n",
    "test_df['label'] = test_df['answer'].apply(process_answer)\n",
    "\n",
    "# Convert to datasets format\n",
    "original_dataset = datasets.DatasetDict({\n",
    "    'train': Dataset.from_pandas(train_df),\n",
    "    'test': Dataset.from_pandas(test_df)\n",
    "})\n",
    "\n",
    "dataset = original_dataset\n",
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a15d97b-70ee-4b67-a537-ddc17c9c525c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.corpus import wordnet\n",
    "\n",
    "# Set a custom directory for NLTK data\n",
    "nltk.data.path.append('./nltk_data')\n",
    "\n",
    "# Test: Access WordNet to confirm it works\n",
    "synsets = wordnet.synsets(\"dog\")\n",
    "print(synsets)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5a31d38-b63a-4d3b-959b-c3b180026a75",
   "metadata": {},
   "source": [
    "# Define model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "477665a8-3e7a-4f91-89f7-3aabdf7d76c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultimodalVQAModel(nn.Module):\n",
    "    def __init__(\n",
    "            self,\n",
    "            num_labels: int = len(answer_space),\n",
    "            intermediate_dim: int = 512,\n",
    "            pretrained_text_name: str = 'bert-base-uncased',\n",
    "            pretrained_image_name: str = 'google/vit-base-patch16-224-in21k'):\n",
    "\n",
    "        super(MultimodalVQAModel, self).__init__()\n",
    "        self.num_labels = num_labels\n",
    "        self.pretrained_text_name = pretrained_text_name\n",
    "        self.pretrained_image_name = pretrained_image_name\n",
    "\n",
    "        self.text_encoder = AutoModel.from_pretrained(\n",
    "            self.pretrained_text_name,\n",
    "        )\n",
    "        self.image_encoder = AutoModel.from_pretrained(\n",
    "            self.pretrained_image_name,\n",
    "        )\n",
    "        self.fusion = nn.Sequential(\n",
    "            nn.Linear(self.text_encoder.config.hidden_size + self.image_encoder.config.hidden_size, intermediate_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.5),\n",
    "        )\n",
    "\n",
    "        self.classifier = nn.Linear(intermediate_dim, self.num_labels)\n",
    "        self.criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "    def forward(\n",
    "            self,\n",
    "            input_ids: torch.LongTensor,\n",
    "            pixel_values: torch.FloatTensor,\n",
    "            attention_mask: Optional[torch.LongTensor] = None,\n",
    "            token_type_ids: Optional[torch.LongTensor] = None,\n",
    "            labels: Optional[torch.LongTensor] = None):\n",
    "\n",
    "        encoded_text = self.text_encoder(\n",
    "            input_ids=input_ids,\n",
    "            attention_mask=attention_mask,\n",
    "            token_type_ids=token_type_ids,\n",
    "            return_dict=True,\n",
    "        )\n",
    "        encoded_image = self.image_encoder(\n",
    "            pixel_values=pixel_values,\n",
    "            return_dict=True,\n",
    "        )\n",
    "        fused_output = self.fusion(\n",
    "            torch.cat(\n",
    "                [\n",
    "                    encoded_text['pooler_output'],\n",
    "                    encoded_image['pooler_output'],\n",
    "                ],\n",
    "                dim=1\n",
    "            )\n",
    "        )\n",
    "        logits = self.classifier(fused_output)\n",
    "\n",
    "        out = {\n",
    "            \"logits\": logits\n",
    "        }\n",
    "        if labels is not None:\n",
    "            loss = self.criterion(logits, labels)\n",
    "            out[\"loss\"] = loss\n",
    "\n",
    "        return out\n",
    "\n",
    "@dataclass\n",
    "class MultimodalCollator:\n",
    "    tokenizer: AutoTokenizer\n",
    "    preprocessor: AutoImageProcessor\n",
    "\n",
    "    def tokenize_text(self, texts: List[str]):\n",
    "        encoded_text = self.tokenizer(\n",
    "            text=texts,\n",
    "            padding='longest',\n",
    "            max_length=24,\n",
    "            truncation=True,\n",
    "            return_tensors='pt',\n",
    "            return_token_type_ids=True,\n",
    "            return_attention_mask=True,\n",
    "        )\n",
    "        return {\n",
    "            \"input_ids\": encoded_text['input_ids'].squeeze(),\n",
    "            \"token_type_ids\": encoded_text['token_type_ids'].squeeze(),\n",
    "            \"attention_mask\": encoded_text['attention_mask'].squeeze(),\n",
    "        }\n",
    "\n",
    "    def preprocess_images(self, images: List[str]):\n",
    "        processed_images = self.preprocessor(\n",
    "            images=[Image.open(os.path.join(\"dataset\", \"images\", image_id + \".png\")).convert('RGB') for image_id in images],\n",
    "            return_tensors=\"pt\",\n",
    "        )\n",
    "        return {\n",
    "            \"pixel_values\": processed_images['pixel_values'].squeeze(),\n",
    "        }\n",
    "\n",
    "    def __call__(self, raw_batch_dict):\n",
    "        return {\n",
    "            **self.tokenize_text(\n",
    "                raw_batch_dict['question']\n",
    "                if isinstance(raw_batch_dict, dict) else\n",
    "                [i['question'] for i in raw_batch_dict]\n",
    "            ),\n",
    "            **self.preprocess_images(\n",
    "                raw_batch_dict['image_id']\n",
    "                if isinstance(raw_batch_dict, dict) else\n",
    "                [i['image_id'] for i in raw_batch_dict]\n",
    "            ),\n",
    "            'labels': torch.tensor(\n",
    "                raw_batch_dict['label']\n",
    "                if isinstance(raw_batch_dict, dict) else\n",
    "                [i['label'] for i in raw_batch_dict],\n",
    "                dtype=torch.int64\n",
    "            ),\n",
    "        }\n",
    "\n",
    "def createMultimodalVQACollatorAndModel(text='bert-base-uncased', image='google/vit-base-patch16-224-in21k'):\n",
    "    tokenizer = AutoTokenizer.from_pretrained(text, use_fast=True)\n",
    "    preprocessor = AutoImageProcessor.from_pretrained(image, use_fast=True)\n",
    "\n",
    "    multi_collator = MultimodalCollator(\n",
    "        tokenizer=tokenizer,\n",
    "        preprocessor=preprocessor,\n",
    "    )\n",
    "\n",
    "    multi_model = MultimodalVQAModel(pretrained_text_name=text, pretrained_image_name=image).to(device)\n",
    "    return multi_collator, multi_model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22d960f3-fbf8-440f-8c1e-218a559bcf98",
   "metadata": {},
   "source": [
    "# Define matrices "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbb67f89-4647-4c9d-b1f2-7f67bbb69878",
   "metadata": {},
   "outputs": [],
   "source": [
    "def wup_measure(a,b,similarity_threshold=0.925):\n",
    "    \"\"\"\n",
    "    Returns Wu-Palmer similarity score.\n",
    "    More specifically, it computes:\n",
    "        max_{x in interp(a)} max_{y in interp(b)} wup(x,y)\n",
    "        where interp is a 'interpretation field'\n",
    "    \"\"\"\n",
    "    def get_semantic_field(a):\n",
    "        weight = 1.0\n",
    "        semantic_field = wordnet.synsets(a,pos=wordnet.NOUN)\n",
    "        return (semantic_field,weight)\n",
    "\n",
    "\n",
    "    def get_stem_word(a):\n",
    "        \"\"\"\n",
    "        Sometimes answer has form word+:wordid.\n",
    "        If so we return word and downweight\n",
    "        \"\"\"\n",
    "        weight = 1.0\n",
    "        return (a,weight)\n",
    "\n",
    "\n",
    "    global_weight=1.0\n",
    "\n",
    "    (a,global_weight_a)=get_stem_word(a)\n",
    "    (b,global_weight_b)=get_stem_word(b)\n",
    "    global_weight = min(global_weight_a,global_weight_b)\n",
    "\n",
    "    if a==b:\n",
    "        # they are the same\n",
    "        return 1.0*global_weight\n",
    "\n",
    "    if a==[] or b==[]:\n",
    "        return 0\n",
    "\n",
    "\n",
    "    interp_a,weight_a = get_semantic_field(a)\n",
    "    interp_b,weight_b = get_semantic_field(b)\n",
    "\n",
    "    if interp_a == [] or interp_b == []:\n",
    "        return 0\n",
    "\n",
    "    # we take the most optimistic interpretation\n",
    "    global_max=0.0\n",
    "    for x in interp_a:\n",
    "        for y in interp_b:\n",
    "            local_score=x.wup_similarity(y)\n",
    "            if local_score > global_max:\n",
    "                global_max=local_score\n",
    "\n",
    "    # we need to use the semantic fields and therefore we downweight\n",
    "    # unless the score is high which indicates both are synonyms\n",
    "    if global_max < similarity_threshold:\n",
    "        interp_weight = 0.1\n",
    "    else:\n",
    "        interp_weight = 1.0\n",
    "\n",
    "    final_score=global_max*weight_a*weight_b*interp_weight*global_weight\n",
    "    return final_score\n",
    "\n",
    "def batch_wup_measure(labels, preds):\n",
    "    wup_scores = [wup_measure(answer_space[label], answer_space[pred]) for label, pred in zip(labels, preds)]\n",
    "    return np.mean(wup_scores)\n",
    "\n",
    "def compute_metrics(eval_tuple: Tuple[np.ndarray, np.ndarray]) -> Dict[str, float]:\n",
    "    logits, labels = eval_tuple\n",
    "    preds = logits.argmax(axis=-1)\n",
    "    return {\n",
    "        \"wups\": batch_wup_measure(labels, preds),\n",
    "        \"acc\": accuracy_score(labels, preds),\n",
    "        \"f1\": f1_score(labels, preds, average='macro')\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74aaf9c4-efa0-45a9-bd34-c6337db8112a",
   "metadata": {},
   "source": [
    "# Train Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "130cea6d-dcc7-45db-b937-1b5c0135bf9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def createAndTrainModel(dataset, args, text_model='bert-base-uncased',\n",
    "                       image_model='google/vit-base-patch16-224-in21k',\n",
    "                       multimodal_model='bert_vit'):\n",
    "    # Create output directory with timestamp to ensure uniqueness\n",
    "    timestamp = time.strftime(\"%Y%m%d-%H%M%S\")\n",
    "    output_dir = os.path.join(\"checkpoint\", multimodal_model, f\"run_{timestamp}\")\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "    collator, model = createMultimodalVQACollatorAndModel(text_model, image_model)\n",
    "\n",
    "    # Create a new TrainingArguments object with unique run name\n",
    "    training_args = TrainingArguments(\n",
    "        output_dir=output_dir,\n",
    "        run_name=f\"run_{timestamp}\",\n",
    "        evaluation_strategy=\"epoch\",\n",
    "        save_strategy=\"epoch\",\n",
    "        load_best_model_at_end=True,\n",
    "        metric_for_best_model=\"wups\",\n",
    "        greater_is_better=True,\n",
    "        per_device_train_batch_size=args.per_device_train_batch_size if hasattr(args, 'per_device_train_batch_size') else 8,\n",
    "        per_device_eval_batch_size=args.per_device_eval_batch_size if hasattr(args, 'per_device_eval_batch_size') else 8,\n",
    "        num_train_epochs=args.num_train_epochs if hasattr(args, 'num_train_epochs') else 3,\n",
    "        learning_rate=args.learning_rate if hasattr(args, 'learning_rate') else 5e-5,\n",
    "        report_to=[],\n",
    "        remove_unused_columns=False,  # Important: Keep all columns\n",
    "    )\n",
    "\n",
    "    # Verify dataset format\n",
    "    print(\"Sample training example:\", dataset['train'][0])\n",
    "    print(\"Sample test example:\", dataset['test'][0])\n",
    "\n",
    "    multi_trainer = Trainer(\n",
    "        model=model,\n",
    "        args=training_args,\n",
    "        train_dataset=dataset['train'],\n",
    "        eval_dataset=dataset['test'],\n",
    "        data_collator=collator,\n",
    "        compute_metrics=compute_metrics,\n",
    "    )\n",
    "\n",
    "    train_multi_metrics = multi_trainer.train()\n",
    "    eval_multi_metrics = multi_trainer.evaluate()\n",
    "\n",
    "    # Save the final model explicitly\n",
    "    model_save_path = os.path.join(output_dir, \"final_model.pt\")\n",
    "    torch.save({\n",
    "        'model_state_dict': model.state_dict(),\n",
    "        'train_metrics': train_multi_metrics,\n",
    "        'eval_metrics': eval_multi_metrics\n",
    "    }, model_save_path)\n",
    "\n",
    "    return collator, model, train_multi_metrics, eval_multi_metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "642ec575-5180-4969-b8b5-1deb53912425",
   "metadata": {},
   "source": [
    "### Set Device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6dcb7a18-0769-4dc8-866d-156e4b1d5cdf",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "device"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f34a12bf-832a-480e-84ed-beb9be550ec0",
   "metadata": {},
   "source": [
    "### train model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81dfefc0-1829-4be7-9fcf-ce60cc1083cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training arguments\n",
    "args = TrainingArguments(\n",
    "    output_dir=\"checkpoint\",\n",
    "    seed=42,\n",
    "    eval_strategy=\"steps\",\n",
    "    eval_steps=100,\n",
    "    logging_strategy=\"steps\",\n",
    "    logging_steps=100,\n",
    "    save_strategy=\"steps\",\n",
    "    save_steps=100,\n",
    "    save_total_limit=3,\n",
    "    metric_for_best_model='wups',\n",
    "    per_device_train_batch_size=16,\n",
    "    per_device_eval_batch_size=16,\n",
    "    remove_unused_columns=False,\n",
    "    num_train_epochs=5,\n",
    "    fp16=False,\n",
    "    dataloader_num_workers=8,\n",
    "    load_best_model_at_end=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc9e8eec-4063-4ce5-b55a-014070e99c35",
   "metadata": {},
   "outputs": [],
   "source": [
    "collator, model, train_multi_metrics, eval_multi_metrics = createAndTrainModel(dataset, args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8627512a-c00e-46d4-b08e-9b7e6272f3bf",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "019cb495-e729-41bf-94fe-24cc2d2451a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = np.random.randint(len(answer_space), size=5)\n",
    "preds = np.random.randint(len(answer_space), size=5)\n",
    "\n",
    "def showAnswers(ids):\n",
    "    print([answer_space[id] for id in ids])\n",
    "\n",
    "showAnswers(labels)\n",
    "showAnswers(preds)\n",
    "\n",
    "print(\"Predictions vs Labels: \", batch_wup_measure(labels, preds))\n",
    "print(\"Labels vs Labels: \", batch_wup_measure(labels, labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf18b418-1fce-46ea-918b-de64033e0bdb",
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_multi_metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7edfb328-fa45-4713-a243-77e3d59aa72b",
   "metadata": {},
   "outputs": [],
   "source": [
    "device"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed14ec7f-cf15-4889-817a-5de2b427022e",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# Load and test model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2bc019bc-30f8-494d-b6cf-ecee8673428e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers.trainer_utils import TrainOutput\n",
    "\n",
    "# Allow TrainOutput to be deserialized safely\n",
    "torch.serialization.add_safe_globals([TrainOutput])\n",
    "\n",
    "model = MultimodalVQAModel()\n",
    "\n",
    "checkpoint_path = os.path.join(\"checkpoint\", \"bert_vit\", \"final\", \"vqa_basic.pt\")\n",
    "if os.path.exists(checkpoint_path):\n",
    "    # Load the checkpoint with map_location to CPU\n",
    "    checkpoint = torch.load(checkpoint_path, map_location=torch.device('cpu'), weights_only=True)\n",
    "    \n",
    "    model.load_state_dict(checkpoint['model_state_dict'])\n",
    "    model.to(device)\n",
    "    \n",
    "    print(\"Model loaded successfully\")\n",
    "    print(\"Final evaluation metrics:\", checkpoint['eval_metrics'])\n",
    "else:\n",
    "    print(\"No saved model found at:\", checkpoint_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5dbd01f4-d416-46c6-90bb-0a6292c25ad9",
   "metadata": {},
   "outputs": [],
   "source": [
    "sample = collator(dataset[\"test\"][1050:1100])\n",
    "\n",
    "input_ids = sample[\"input_ids\"].to(device)\n",
    "token_type_ids = sample[\"token_type_ids\"].to(device)\n",
    "attention_mask = sample[\"attention_mask\"].to(device)\n",
    "pixel_values = sample[\"pixel_values\"].to(device)\n",
    "labels = sample[\"labels\"].to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84c2f39e-e29d-437b-a8fc-d4df9cfd6b5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.eval()\n",
    "output = model(input_ids, pixel_values, attention_mask, token_type_ids, labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31d47dcf-3fbc-4c54-8c69-f2337c4ef4bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "preds = output[\"logits\"].argmax(axis=-1).cpu().numpy()\n",
    "preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "703ef3fe-fc27-4541-8dcd-1ff1cc77efdc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import display\n",
    "\n",
    "def showExample(train=True, id=None):\n",
    "    if train:\n",
    "        data = dataset[\"train\"]\n",
    "    else:\n",
    "        data = dataset[\"test\"]\n",
    "    if id == None:\n",
    "        id = np.random.randint(len(data))\n",
    "    image = Image.open(os.path.join(\"dataset\", \"images\", data[id][\"image_id\"] + \".png\"))\n",
    "    display(image)\n",
    "\n",
    "    print(\"Question:\\t\", data[id][\"question\"])\n",
    "    print(\"Answer:\\t\\t\", data[id][\"answer\"], \"(Label: {0})\".format(data[id][\"label\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5eefccf9-13f9-40eb-8f78-7258cb879780",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(000, 005):\n",
    "    print(\"*********************************************************\")\n",
    "    showExample(train=False, id=i)\n",
    "    print(\"Predicted Answer:\\t\", answer_space[preds[i-1000]])\n",
    "    print(\"*********************************************************\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47ee7523-b058-4548-8885-5c7183139966",
   "metadata": {},
   "source": [
    "# without training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d0d6f0f-132b-4c5f-b8ae-478510b39a31",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b2968ee-43b3-414a-91a0-d47223d7c23c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Allow TrainOutput to be deserialized safely\n",
    "torch.serialization.add_safe_globals([TrainOutput])\n",
    "\n",
    "test_dataset = dataset['test']\n",
    "checkpoint_path = os.path.join(\"checkpoint\", \"bert_vit\", \"final\", \"vqa_basic.pt\")\n",
    "\n",
    "\n",
    "def createCollatorAndLoadModel(checkpoint_path, text_model='bert-base-uncased',\n",
    "                             image_model='google/vit-base-patch16-224-in21k',\n",
    "                             multimodal_model='bert_vit'):\n",
    "    \n",
    "    collator, model = createMultimodalVQACollatorAndModel(text_model, image_model)\n",
    " \n",
    "    if os.path.exists(checkpoint_path):\n",
    "        # checkpoint = torch.load(checkpoint_path, map_location=torch.device('cpu'), weights_only=True)\n",
    "        checkpoint = torch.load(checkpoint_path, map_location=device, weights_only=True)\n",
    "        print(checkpoint['eval_metrics'])\n",
    "        model.load_state_dict(checkpoint['model_state_dict'])\n",
    "        model.to(device) #\n",
    "        \n",
    "    return collator, model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00612e59-c7e7-4ad4-acfd-dd8b6185b262",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_inference(model, collator, test_dataset, start_idx, end_idx, device=device):\n",
    "\n",
    "    sample = collator(test_dataset[start_idx:end_idx])\n",
    "    \n",
    "    # Move everything to device\n",
    "    input_ids = sample[\"input_ids\"].to(device)\n",
    "    token_type_ids = sample[\"token_type_ids\"].to(device)\n",
    "    attention_mask = sample[\"attention_mask\"].to(device)\n",
    "    pixel_values = sample[\"pixel_values\"].to(device)\n",
    "    labels = sample[\"labels\"].to(device)\n",
    "    \n",
    "    # Set model to evaluation mode\n",
    "    model.eval()\n",
    "    \n",
    "    # Run inference\n",
    "    with torch.no_grad():\n",
    "        output = model(input_ids, pixel_values, attention_mask, token_type_ids, labels)\n",
    "    \n",
    "    # Get predictions\n",
    "    preds = output[\"logits\"].argmax(axis=-1).cpu().numpy()\n",
    "    \n",
    "    # Get actual labels\n",
    "    actual_labels = [test_dataset[i][\"label\"] for i in range(start_idx, end_idx)]\n",
    "    \n",
    "    return preds, actual_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "031f4acb-ba0c-4095-8538-2397592ac002",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model(checkpoint_path, test_dataset, start_idx, end_idx):\n",
    "    \"\"\"\n",
    "    Evaluate a saved model on a test_dataset slice\n",
    "    \"\"\"\n",
    "    # Create collator and load model\n",
    "    collator, model = createCollatorAndLoadModel(checkpoint_path)\n",
    "    model = model.to(device)\n",
    "    \n",
    "    # Run inference\n",
    "    preds, labels = run_inference(model, collator, test_dataset, start_idx, end_idx)\n",
    "    \n",
    "    # Show results\n",
    "    print(\"Predictions:\", [answer_space[id] for id in preds])\n",
    "    print(\"Ground Truth:\", [answer_space[id] for id in labels])\n",
    "    print(\"Predictions vs Labels:\", batch_wup_measure(labels, preds))\n",
    "    print(\"Labels vs Labels:\", batch_wup_measure(labels, labels))\n",
    "    \n",
    "    return preds, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a038ba0-a928-45e9-b3dc-03fd8b4f68ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "def display_results(checkpoint_path, test_dataset, start_idx=100, end_idx=105):\n",
    "    collator, model = createCollatorAndLoadModel(checkpoint_path)\n",
    "    model = model.to(device)\n",
    "    \n",
    "    # Run inference\n",
    "    preds, labels = run_inference(model, collator, test_dataset, start_idx, end_idx)\n",
    "\n",
    "    for i in range(start_idx, end_idx):\n",
    "        image = Image.open(os.path.join(\"dataset\", \"images\", test_dataset[i][\"image_id\"] + \".png\"))\n",
    "        display(image)\n",
    "\n",
    "        print(\"Question:\\t\", test_dataset[i][\"question\"])\n",
    "        # print(\"Answer:\\t\\t\", test_dataset[i][\"answer\"], \"(Label: {0})\".format(test_dataset[i][\"label\"]))\n",
    "        print(\"Ground truth:\\t\\t\", answer_space[labels[i-start_idx]], \"(Label: {0})\".format(labels[i-start_idx]))\n",
    "        print(\"Prediction: \\t\\t\", answer_space[preds[i-start_idx]], \"(Label: {0})\\n\\n\".format(preds[i-start_idx]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9c12f6b-b4eb-4038-87bb-a89735b2acd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint_path = os.path.join(\"checkpoint\", \"bert_vit\", \"final\", \"vqa_basic.pt\")\n",
    "preds, labels = evaluate_model(checkpoint_path, test_dataset, start_idx=100, end_idx=105)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "345e52ad-1d9f-4439-85d7-065cbada7b0a",
   "metadata": {},
   "source": [
    "## Display batch results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1acaacb5-ba1c-4c36-99ff-c1fc339eb400",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "checkpoint_path = os.path.join(\"checkpoint\", \"bert_vit\", \"final\", \"vqa_basic.pt\")\n",
    "display_results(checkpoint_path, test_dataset, start_idx=250, end_idx=260)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "344a7a75-9af7-4211-a904-abe8a33be1eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "collator, model = createCollatorAndLoadModel(checkpoint_path)\n",
    "sample = collator(test_dataset[100:101])\n",
    "sample[\"input_ids\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96878985-a206-4cd3-9462-341312b69350",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ask_question(checkpoint_path, image_id, question):\n",
    "    test_data = {\n",
    "    'question': [f'{question}', 'dummy'],\n",
    "    'image_id': [f'image{image_id}', 'image436'],\n",
    "    'label': [483, 0]\n",
    "    }\n",
    "    \n",
    "    collator, model = createCollatorAndLoadModel(checkpoint_path)\n",
    "    model = model.to(device)\n",
    "    \n",
    "    # Run inference\n",
    "    sample = collator(test_data)\n",
    "    ## Move everything to device\n",
    "    input_ids = sample[\"input_ids\"].to(device)\n",
    "    token_type_ids = sample[\"token_type_ids\"].to(device)\n",
    "    attention_mask = sample[\"attention_mask\"].to(device)\n",
    "    pixel_values = sample[\"pixel_values\"].to(device)\n",
    "    labels = sample[\"labels\"].to(device)\n",
    "    \n",
    "    ## Set model to evaluation mode\n",
    "    model.eval()\n",
    "    \n",
    "    ## Run inference\n",
    "    with torch.no_grad():\n",
    "        output = model(input_ids, pixel_values, attention_mask, token_type_ids, labels)\n",
    "    \n",
    "    ## Get predictions\n",
    "    preds = output[\"logits\"].argmax(axis=-1).cpu().numpy()\n",
    "    \n",
    "    # preds = run_inference(model, collator, test_data)\n",
    "\n",
    "    image = Image.open(os.path.join(\"dataset\", \"images\", f'image{image_id}' + \".png\"))\n",
    "    display(image)\n",
    "    print(\"Question:\\t\", question)\n",
    "    print(\"Prediction: \\t\\t\", answer_space[preds[0]], \"(Label: {0})\\n\\n\".format(preds[0]))\n",
    "\n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2489437b-d0a7-44e8-acfa-a23a835c81cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "ask_question(checkpoint_path, question='what is at the center of the cabinet?',image_id=436)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2a2ade0-da21-4a4d-b117-66a321ec6e7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint_path = os.path.join(\"checkpoint\", \"bert_vit\", \"final\", \"vqa_basic.pt\")\n",
    "display_results(checkpoint_path, test_dataset, start_idx=150, end_idx=200)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae4e8cfd-a2a9-4ae5-a642-d7e3c0173394",
   "metadata": {},
   "source": [
    "# "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2313bd9-4c75-4848-a215-cdb32d361a76",
   "metadata": {},
   "source": [
    "# "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (AI)",
   "language": "python",
   "name": "ai"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
